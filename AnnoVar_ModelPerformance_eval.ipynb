{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Annotator Variability and Model Performance Evaluation\n",
    "This notebook is part of the paper: Automated Segmentation of the Dorsal Root Ganglia (DRG) in MRI by Nauroth-Kreß et al., 2024\n",
    "The following cells contain the code used for the cacluctation of the inter-annotator metric scores and the model segmentation performance evaluation. After the cells for calculations folow cells with the matching visualization code.\n",
    "\n",
    "The code is ready to use with any dataset matching the general structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from itertools import combinations\n",
    "from pathlib import Path\n",
    "from surface_distance import compute_dice_coefficient, compute_surface_distances, compute_average_surface_distance\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "\n",
    "def load_files(data_dir : Path | str) -> List[nib.Nifti1Image]:\n",
    "    \"\"\"Load all nifti files in directory\"\"\"\n",
    "    file_paths = sorted(Path(data_dir).glob('*.nii*'))\n",
    "    imgs = {Path(path.stem).stem.split('_')[0]: nib.load(path) for path in file_paths}\n",
    "    return imgs\n",
    "\n",
    "\n",
    "def calc_metrics(gt : nib.Nifti1Image, pred : nib.Nifti1Image, labels : List[int] = None, spacing : List[int] = None) -> dict:\n",
    "    \"\"\"Calculate the DSC and ASD for the a prediction ground truth pair.\n",
    "    :param labels: List of valid label values, default: None -> all valid.\n",
    "    :param spacing: Voxel spacing (x,y,z), default: None -> take from gt header\n",
    "    :return: Dictionary with metric scores per label ready for conversion to a pd.DataFrame\n",
    "    \"\"\"\n",
    "    if not labels:\n",
    "        labels = np.unique(gt.get_fdata())\n",
    "    if not spacing:\n",
    "        spacing = np.round(gt.header.get_zooms(), 2)\n",
    "        if gt.header.get_zooms() != pred.header.get_zooms():\n",
    "            raise ValueError('Image spacings do not match!')\n",
    "    res = dict(\n",
    "        Label=[],\n",
    "        DSC=[],\n",
    "        ASD=[],\n",
    "    )\n",
    "    for label in labels:\n",
    "        if label != 0:\n",
    "            gt_mask = gt.get_fdata() == label\n",
    "            pred_mask = pred.get_fdata() == label\n",
    "            dice = compute_dice_coefficient(gt_mask, pred_mask)\n",
    "            sdist = compute_surface_distances(gt_mask, pred_mask, spacing_mm=spacing)\n",
    "            avsdist = compute_average_surface_distance(sdist)[1]\n",
    "            res['Label'] += [label]\n",
    "            res['DSC'] += [dice]\n",
    "            res['ASD'] += [avsdist]\n",
    "    return res\n",
    "\n",
    "\n",
    "def eval_model(gt : Dict[str, nib.Nifti1Image], pred : Dict[str, nib.Nifti1Image], valid_labels : List[int] = None, vspacing : List[int] = None) -> pd.DataFrame:\n",
    "    \"\"\"Calculate evaluation metrics for each prediction ground truth pair.\n",
    "    Checks the pairing before calculation.\n",
    "    :param gt: Dictinary containg subject IDs as keys and the corresponding GT segmentations as values.\n",
    "    :param pred: Dictinary containg IDs as keys and the corresponding predicted segmentations as values.\n",
    "    :param valid_labels: List of valid label values, default: None -> all valid.\n",
    "    :param vspacing: Voxel spacing (x,y,z), default: None -> take from gt header\n",
    "    :return: Dataframe with all results. Identification via subject IDs and label columns.\n",
    "    \"\"\"\n",
    "    if len(gt) != len(pred):\n",
    "        raise ValueError('Number of GT and Pred do not match!')\n",
    "    \n",
    "    results = []\n",
    "    for (gt_key, gt_img), (pred_key, pred_img) in zip(gt.items(), pred.items()):\n",
    "        # check if the IDs match\n",
    "        if gt_key != pred_key:\n",
    "            raise ValueError(f'Img IDs do not match: {gt_key} <> {pred_key}!')\n",
    "        metrics = calc_metrics(gt_img, pred_img, labels=valid_labels, spacing=vspacing)\n",
    "        tmp_dict = dict(\n",
    "            Sub_ID = [gt_key for i in range(len(metrics['Label']))],\n",
    "            **metrics\n",
    "        )\n",
    "        results.append(pd.DataFrame.from_dict(tmp_dict))\n",
    "    \n",
    "    return pd.concat(results)\n",
    "\n",
    "\n",
    "def eval_anno_var(ds_dir: Path | str) -> pd.DataFrame:\n",
    "    \"\"\"Calcluate metrics for all possible annotator combinations.\n",
    "    The annotator segmentation images have to be in annotator specific subdirectories.\n",
    "    :param ds_dir: Path to directoy containing annotator subdirectories.\n",
    "    :return: Dataframe with all metrics for each possile anotator pair.\n",
    "    \"\"\"\n",
    "    anno_segs = {anno_dir.name: load_files(anno_dir) for anno_dir in Path(ds_dir, 'annotator_labels').iterdir() if anno_dir.is_dir()}\n",
    "    all_combs = combinations(anno_segs, 2)\n",
    "    all_comb_dfs = {f'{a}-{b}': eval_model(anno_segs[a], anno_segs[b]) for a, b in all_combs}\n",
    "    out_df = pd.concat(all_comb_dfs.values(), keys=all_comb_dfs.keys(), names=['Combination']).reset_index(level='Combination')\n",
    "    return out_df\n",
    "\n",
    "\n",
    "def eval_on_dataset(ds_dir : Path | str) -> pd.DataFrame:\n",
    "    \"\"\"Caculate metrics for al models on a data set.\n",
    "    Dataset directory must contain a model subdirectory containing a ground_truth and a predictions subdirectory.\n",
    "    :param ds_dir: Path to directoy containing model subdirectories.\n",
    "    :return: Dataframe with all metrics for each model.\n",
    "    \"\"\"\n",
    "    model_preds = {model_dir.name: load_files(model_dir) for model_dir in Path(ds_dir, 'model_predictions').iterdir() if model_dir.is_dir()}\n",
    "    gt = load_files(Path(ds_dir, 'staple_gt'))\n",
    "    model_dfs = {model: eval_model(gt, pred) for model, pred in model_preds.items()}\n",
    "    out_df =  pd.concat(model_dfs.values(), keys=model_dfs.keys(), names=['Model']).reset_index(level='Model')\n",
    "    return out_df\n",
    "\n",
    "\n",
    "def calc_mean_scores(df, col1, col2, metrics=['DSC', 'ASD'], round=2, total_mean_col=None) -> pd.DataFrame | Tuple[pd.DataFrame]:\n",
    "    \"\"\"Calculate the mean for a subset defined by two identification columns.\n",
    "    :param df: Dataframe with two identification columns and an arbitrary number of value columns\n",
    "    :param col1: Name of first level identification column\n",
    "    :param col2: Name of second level identification column\n",
    "    :param metrics: Name of value / metric columns to calculate mean on\n",
    "    :param round: Number of decimal places\n",
    "    :param total_mean_col: Name of first level identification column to calculate mean over all second level subsets\n",
    "    :return: Dataframe with the mean values per metric and subsets | Tuple of Dataframe with the mean values per metric and second level subsets and Dataframe with the mean values per metric and first level subsets\n",
    "    \"\"\"\n",
    "    res_list = []\n",
    "    for c1 in df[col1].unique():\n",
    "        for c2 in df[col2].unique():\n",
    "            tmp_df = df[\n",
    "                (df[col1]==c1) &\n",
    "                (df[col2]==c2)\n",
    "            ][metrics]\n",
    "            tmp_dict = dict(\n",
    "                **{col1: c1, col2: c2},\n",
    "                **{metric: tmp_df[metric].mean().round(round) for metric in metrics}\n",
    "            )\n",
    "            res_list.append(tmp_dict)\n",
    "    res = pd.DataFrame.from_records(res_list)\n",
    "    if total_mean_col:\n",
    "        total_res_list = []\n",
    "        for c3 in res[total_mean_col].unique():\n",
    "            tmp_dict = dict(\n",
    "                **{total_mean_col: c3},\n",
    "                **{metric: res[res[total_mean_col]==c3][metric].mean().round(round) for metric in metrics}\n",
    "            )\n",
    "            total_res_list.append(tmp_dict)\n",
    "        return res, pd.DataFrame.from_records(total_res_list)\n",
    "    return res"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Annotator Variability\n",
    "Execute the following cell to compare the variability between annotators by calculating the metric scores for each possible annotator pair on all labels.\n",
    "Input the path to the directory containing the testset directories.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metric scores all annotator combinations\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DataSet</th>\n",
       "      <th>Combination</th>\n",
       "      <th>DSC</th>\n",
       "      <th>ASD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HE</td>\n",
       "      <td>annotator2-annotator1</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HE</td>\n",
       "      <td>annotator2-annotator3</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HE</td>\n",
       "      <td>annotator1-annotator3</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FD</td>\n",
       "      <td>annotator2-annotator1</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FD</td>\n",
       "      <td>annotator2-annotator3</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>FD</td>\n",
       "      <td>annotator1-annotator3</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  DataSet            Combination   DSC   ASD\n",
       "0      HE  annotator2-annotator1  0.83  0.36\n",
       "1      HE  annotator2-annotator3  0.85  0.28\n",
       "2      HE  annotator1-annotator3  0.92  0.13\n",
       "3      FD  annotator2-annotator1  0.85  0.34\n",
       "4      FD  annotator2-annotator3  0.85  0.25\n",
       "5      FD  annotator1-annotator3  0.90  0.13"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean metric scores over all annotators\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DataSet</th>\n",
       "      <th>DSC</th>\n",
       "      <th>ASD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HE</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FD</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  DataSet   DSC   ASD\n",
       "0      HE  0.87  0.26\n",
       "1      FD  0.87  0.24"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_dir = Path(input('Input paths to the directory containing test set subdirectories:\\n'))\n",
    "\n",
    "# evaluate on test sets\n",
    "ts = {path.name: eval_anno_var(path) for path in data_dir.iterdir() if path.is_dir()}\n",
    "av_full = pd.concat(\n",
    "    ts.values(),\n",
    "    keys=ts.keys(), \n",
    "    names=['DataSet']\n",
    ").reset_index(level='DataSet')\n",
    "\n",
    "# calculate the mean scores\n",
    "res, total_res = calc_mean_scores(av_full, 'DataSet', 'Combination', total_mean_col='DataSet')\n",
    "print('Metric scores all annotator combinations')\n",
    "display(res)\n",
    "print('Mean metric scores over all annotators')\n",
    "display(total_res)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the results as .csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "av_full.to_csv(Path(input('Input save path:\\n'), 'annotator_variability_scores.csv'), index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Performance Evaluation\n",
    "### Calculation\n",
    "Execute the following cell to calculate DSC and ASD for the test set labels predicted by each model.\n",
    "Input the path to the directory containing the test set subdirectories.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detail metric scores\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DataSet</th>\n",
       "      <th>Model</th>\n",
       "      <th>Sub_ID</th>\n",
       "      <th>Label</th>\n",
       "      <th>DSC</th>\n",
       "      <th>ASD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HE</td>\n",
       "      <td>DC-CE-LCD</td>\n",
       "      <td>Sub-Ctrl007</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.087373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HE</td>\n",
       "      <td>DC-CE-LCD</td>\n",
       "      <td>Sub-Ctrl007</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.929843</td>\n",
       "      <td>0.090706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HE</td>\n",
       "      <td>DC-CE-LCD</td>\n",
       "      <td>Sub-Ctrl007</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.897041</td>\n",
       "      <td>0.121676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HE</td>\n",
       "      <td>DC-CE-LCD</td>\n",
       "      <td>Sub-Ctrl007</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.945571</td>\n",
       "      <td>0.063960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HE</td>\n",
       "      <td>DC-CE-LCD</td>\n",
       "      <td>Sub-Ctrl015</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.786760</td>\n",
       "      <td>0.674039</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  DataSet      Model       Sub_ID  Label       DSC       ASD\n",
       "0      HE  DC-CE-LCD  Sub-Ctrl007    1.0  0.923077  0.087373\n",
       "1      HE  DC-CE-LCD  Sub-Ctrl007    2.0  0.929843  0.090706\n",
       "2      HE  DC-CE-LCD  Sub-Ctrl007    3.0  0.897041  0.121676\n",
       "3      HE  DC-CE-LCD  Sub-Ctrl007    4.0  0.945571  0.063960\n",
       "0      HE  DC-CE-LCD  Sub-Ctrl015    1.0  0.786760  0.674039"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean metric scores of all models\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DataSet</th>\n",
       "      <th>Model</th>\n",
       "      <th>DSC</th>\n",
       "      <th>ASD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HE</td>\n",
       "      <td>DC-CE-LCD</td>\n",
       "      <td>0.88</td>\n",
       "      <td>4.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HE</td>\n",
       "      <td>DC-TopK</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HE</td>\n",
       "      <td>DC-CE</td>\n",
       "      <td>0.85</td>\n",
       "      <td>8.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FD</td>\n",
       "      <td>DC-CE-LCD</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FD</td>\n",
       "      <td>DC-TopK</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>FD</td>\n",
       "      <td>DC-CE</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  DataSet      Model   DSC   ASD\n",
       "0      HE  DC-CE-LCD  0.88  4.04\n",
       "1      HE    DC-TopK  0.88  0.19\n",
       "2      HE      DC-CE  0.85  8.60\n",
       "3      FD  DC-CE-LCD  0.88  0.16\n",
       "4      FD    DC-TopK  0.89  0.16\n",
       "5      FD      DC-CE  0.89  0.17"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dir = Path(input('Input paths to the directory containing test set subdirectories:\\n'))\n",
    "\n",
    "# evaluate on test sets\n",
    "ts = {path.name: eval_on_dataset(path) for path in data_dir.iterdir() if path.is_dir()}\n",
    "eval_full = pd.concat(\n",
    "    ts.values(), \n",
    "    keys=ts.keys(), \n",
    "    names=['DataSet']\n",
    ").reset_index(level='DataSet')\n",
    "print('Detail metric scores')\n",
    "display(eval_full.head())\n",
    "\n",
    "# calculate the mean scores\n",
    "print('Mean metric scores of all models')\n",
    "calc_mean_scores(eval_full, 'DataSet', 'Model')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the results as .csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_full.to_csv(Path(input('Input save path: ', 'model_performance_scores.csv')), index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from matplotlib.patches import PathPatch\n",
    "\n",
    "\n",
    "# taken from the answer of Thomas Kühn (Jan 31, 2018 at 11:41) on the stackoverflow thread: \n",
    "# Set space between boxplots in Python Graphs generated nested box plots with Seaborn? \n",
    "def adjust_box_widths(g, fac):\n",
    "    \"\"\"\n",
    "    Adjust the withs of a seaborn-generated boxplot.\n",
    "    \"\"\"\n",
    "\n",
    "    # iterating through Axes instances\n",
    "    for ax in g.axes:\n",
    "\n",
    "        # iterating through axes artists:\n",
    "        for c in ax.get_children():\n",
    "\n",
    "            # searching for PathPatches\n",
    "            if isinstance(c, PathPatch):\n",
    "                # getting current width of box:\n",
    "                p = c.get_path()\n",
    "                verts = p.vertices\n",
    "                verts_sub = verts[:-1]\n",
    "                xmin = np.min(verts_sub[:, 0])\n",
    "                xmax = np.max(verts_sub[:, 0])\n",
    "                xmid = 0.5*(xmin+xmax)\n",
    "                xhalf = 0.5*(xmax - xmin)\n",
    "\n",
    "                # setting new width of box\n",
    "                xmin_new = xmid-fac*xhalf\n",
    "                xmax_new = xmid+fac*xhalf\n",
    "                verts_sub[verts_sub[:, 0] == xmin, 0] = xmin_new\n",
    "                verts_sub[verts_sub[:, 0] == xmax, 0] = xmax_new\n",
    "\n",
    "                # setting new width of median line\n",
    "                for l in ax.lines:\n",
    "                    if np.all(l.get_xdata() == [xmin, xmax]):\n",
    "                        l.set_xdata([xmin_new, xmax_new])\n",
    "\n",
    "\n",
    "def scatterbox(data, x, y, hue: str = None, order : List[str] = None, palette : list = None, broken : bool = None, xlabel : str = None, ylabel : str = None, legend : bool = True, ltitle : str = None, lloc : str = 'best', labels : List[str] = None, ylim : Tuple[float, float] = None, font_scale : int = 1, despine : bool = None, figsize : Tuple[int, int] = None):\n",
    "    \"\"\"Combined box and strip plot with or without broken y axis.\n",
    "\n",
    "    :param data: pd.DataFrame containing the data\n",
    "    :param x: Column name to plot as x\n",
    "    :param y: Column name to plot as y\n",
    "    :param hue: Column name hue encoding, default: None -> no hue encoding\n",
    "    :param order: X order, default: None -> derive from dataframe\n",
    "    :param palette: Color palette, default: None -> seaborn default palette\n",
    "    :parm broken: Toggle broken axis, default: False\n",
    "    :param xlabel: X axis label, default: None -> column name as label\n",
    "    :param ylabel: Y axis label, default: None -> column name as label\n",
    "    :param legend: Toggle legend, default: True\n",
    "    :param ltitle: Legend title, default: None\n",
    "    :param lloc: Legend location, default: 'best'\n",
    "    :param labels: Legend labels, default: None -> derive from dataframe\n",
    "    :param ylim: Y axis limits, default: None -> seaborn auto axis limits\n",
    "    :param font_scale: Font scaling factor, default: 1\n",
    "    :param despine: Toggle despine, default: False\n",
    "    :param figsize: Figure size (width, height)\n",
    "    :return: Figure object, axe object\n",
    "    \"\"\"\n",
    "    count = len(data[hue].unique())\n",
    "    # scale all plot fonts\n",
    "    sns.set(font_scale=font_scale)\n",
    "    sns.set_style('ticks')\n",
    "    \n",
    "    # we need two axe objects for a broken y axis\n",
    "    if broken:\n",
    "        fig,(ax,ax2) = plt.subplots(2, 1, figsize=figsize, sharex=True, height_ratios=broken['height_ratio'] if 'height_ratio' in broken else (abs(broken['bot_lim'][0])/abs(broken['bot_lim'][1]), abs(broken['top_lim'][0])/abs(broken['top_lim'][1])))\n",
    "        ax.spines['bottom'].set_visible(False)\n",
    "        ax2.spines['top'].set_visible(False)\n",
    "    else:\n",
    "        fig, ax = plt.subplots(1,1, figsize=figsize)\n",
    "\n",
    "    # generate a box plot from the data\n",
    "    plot = sns.boxplot(\n",
    "        x=x, y=y, hue=hue, data=data, order=order,\n",
    "        palette=palette, \n",
    "        ax=ax,\n",
    "    )\n",
    "    # overlay with a strip plot\n",
    "    plot = sns.stripplot(\n",
    "        x=x, y=y, hue=hue, data=data, order=order,\n",
    "        jitter=True, dodge=True, marker='o', edgecolor='black', linewidth=1, alpha=0.7, palette=['white', 'white'],\n",
    "        ax=ax,\n",
    "    )\n",
    "    \n",
    "    # reformat the axes of the two axe objects (axo)\n",
    "    if broken:\n",
    "        # remove the x axis and set y range - upper axo\n",
    "        plot.set(\n",
    "            xlabel=None,\n",
    "            xticklabels=[],\n",
    "            ylabel=None,\n",
    "            ylim=broken['top_lim'],\n",
    "        )\n",
    "        # specify custom y ticks - upper axo\n",
    "        if 'top_ticks' in broken:\n",
    "            plot.set(\n",
    "                yticks=broken['top_ticks']\n",
    "            )\n",
    "        plot.tick_params(bottom=False)\n",
    "        # fill lower axo with the same data\n",
    "        plot2 = sns.boxplot(\n",
    "            x=x, y=y, hue=hue, data=data, order=order,\n",
    "            palette=palette, \n",
    "            ax=ax2,\n",
    "        )\n",
    "        plot2 = sns.stripplot(\n",
    "            x=x, y=y, hue=hue, data=data, order=order,\n",
    "            jitter=True, dodge=True, marker='o', edgecolor='black', linewidth=1, alpha=0.7, palette=['white', 'white'],\n",
    "            ax=ax2,\n",
    "        )\n",
    "        # set y range - lower axo\n",
    "        plot2.set(\n",
    "            ylabel=None,\n",
    "            ylim=broken['bot_lim'],\n",
    "        )\n",
    "        # specify custom y ticks - lower axo\n",
    "        if 'bot_ticks' in broken:\n",
    "            plot2.set(\n",
    "                yticks=broken['bot_ticks']\n",
    "            )\n",
    "        # set custom x label\n",
    "        if xlabel:\n",
    "            if xlabel=='off':\n",
    "                plot2.set(xlabel=None)\n",
    "            else:\n",
    "                plot2.set(xlabel=xlabel)\n",
    "        # set custom y label\n",
    "        if ylabel:\n",
    "            if ylabel != 'off':\n",
    "                plt.annotate(ylabel, (0.025, 0.5), xytext=(0.015, 0.5), rotation=90, xycoords='figure fraction', va='center')\n",
    "        else:\n",
    "            plt.annotate(y, (0.025, 0.5), xytext=(0.045, 0.5), rotation=90, xycoords='figure fraction', va='center')\n",
    "\n",
    "        d = .5  # proportion of vertical to horizontal extent of the slanted line\n",
    "\n",
    "        # add boken axe markers\n",
    "        kwargs = dict(\n",
    "            marker=[(-1, -d), (1, d)], markersize=12,\n",
    "            linestyle=\"none\", color='k', mec='k', mew=1, clip_on=False\n",
    "        )\n",
    "        if despine:\n",
    "            ax.plot([0, ], [0, ], transform=ax.transAxes, **kwargs)\n",
    "            ax2.plot([0, ], [1, ], transform=ax2.transAxes, **kwargs)\n",
    "        else:\n",
    "            ax.plot([0, 1], [0, 0], transform=ax.transAxes, **kwargs)\n",
    "            ax2.plot([0, 1], [1, 1], transform=ax2.transAxes, **kwargs)\n",
    "    else:\n",
    "        plot.set(\n",
    "            ylim=ylim\n",
    "        )\n",
    "        # set custom x label\n",
    "        if xlabel:\n",
    "            if xlabel == 'off':\n",
    "                plot.set(xlabel=None)\n",
    "            else:\n",
    "                plot.set(xlabel=xlabel)\n",
    "        # set custom y label\n",
    "        if ylabel:\n",
    "            if ylabel == 'off':\n",
    "                plot.set(ylabel=None)\n",
    "            else:\n",
    "                plot.set(ylabel=ylabel)\n",
    "    # dspine toggle\n",
    "    if despine:\n",
    "        if broken:\n",
    "            ax.spines['top'].set_visible(False)\n",
    "            ax.spines['right'].set_visible(False)\n",
    "            ax2.spines['right'].set_visible(False)\n",
    "        else:\n",
    "            sns.despine(right=True)\n",
    "    \n",
    "    # add plot legend\n",
    "    ax.get_legend().remove()\n",
    "    if broken:\n",
    "        ax2.get_legend().remove()\n",
    "    if legend:\n",
    "        handles, _labels = ax.get_legend_handles_labels()\n",
    "        if not labels:\n",
    "            labels=_labels\n",
    "        if lloc=='best':\n",
    "            ax.legend(handles, labels, loc=lloc, title=ltitle, frameon=True)\n",
    "            fig.tight_layout()\n",
    "        else:\n",
    "            fig.legend(handles, labels, loc=lloc, title=ltitle, frameon=False)\n",
    "            fig.tight_layout()\n",
    "            if lloc==7:\n",
    "                fig.subplots_adjust(right=0.75)\n",
    "    else:\n",
    "        fig.tight_layout()\n",
    "    if broken:\n",
    "        fig.subplots_adjust(hspace=0.05)\n",
    "    return fig, plot\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute the following cell to display the plots in a matplotlib pop-up window.<br>\n",
    "The window allows for manual modifications of border width etc. and saving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have to either execute the calculation cell above or load the results from a .csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_full = pd.read_csv(input('Path to model evaluation result file:\\n'))\n",
    "eval_full.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dice Similarity Coefficient (DSC)\n",
    "For the paper figures the following attributes were adjusted in the pop-up window:\n",
    "- top=0.95\n",
    "- bottom=0.15\n",
    "- left=0.17\n",
    "- right=0.96"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = eval_full\n",
    "\n",
    "fig, plot = scb_dsc = scatterbox(\n",
    "    data=df, x=\"Model\", y=\"DSC\", hue='DataSet', order=['DC-CE', 'DC-CE-LCD', 'DC-TopK'],\n",
    "    palette='Greys', \n",
    "    xlabel='Model', ylabel='DSC', ltitle= 'Test Set', labels=['HE', 'FD'],\n",
    "    broken=dict(bot_lim=(-0.05,0.15), top_lim=(0.45,1), bot_ticks=[0,0.1], height_ratio=(10,3)),\n",
    "    font_scale=1.4,\n",
    "    despine=True,\n",
    "    lloc='best',\n",
    "    figsize=(5,5)\n",
    ")\n",
    "adjust_box_widths(fig, 0.9)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Average Surface Distance (ASD)\n",
    "For the paper figures the following attributes were adjusted in the pop-up window:\n",
    "- top=0.95\n",
    "- bottom=0.15\n",
    "- left=0.17\n",
    "- right=0.96"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = eval_full\n",
    "\n",
    "fig, plot = scatterbox(\n",
    "    data=df, x=\"Model\", y=\"ASD\", hue='DataSet', order=['DC-CE', 'DC-CE-LCD', 'DC-TopK'],\n",
    "    palette='Greys',\n",
    "    xlabel='Model', ylabel='ASD', ltitle='Test Set', labels=['HE', 'FD'],\n",
    "    font_scale=1.4,\n",
    "    broken=dict(top_lim=(75,225), bot_lim=(0,1.1), top_ticks=[100,225], height_ratio=(1,10)),\n",
    "    despine=True,\n",
    "    lloc='best',\n",
    "    figsize=(5,5)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fim",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9c86fd167394a4f75a14aa72f3d9242fb612dfabe6027dfb3bb945cf14260bd9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
